{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab1_ML_Security.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NuzQWjC2BrQ",
        "outputId": "48f727e9-ab20-4fa7-e5b0-789670404e3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfiuPmd05rKT"
      },
      "source": [
        "import math\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from os import scandir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPrwResmv2RR"
      },
      "source": [
        "Generating FeatureSet!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4rfCnwh40T4"
      },
      "source": [
        "# accessing files from the directory and creating a SET a feature set by iterating through all the variables\n",
        "\n",
        "dirpath = '/content/drive/My Drive/lingspam_public/lemm_stop/'\n",
        "\n",
        "#Cleanerlist = (\".\",\"\\'\",\"\\\"\",\"^\",\")\",\"(\",\"{\",\"}\",\",\",\"/\",\"?\",\";\",\"\\\"\")\n",
        "\n",
        "# Universal Feature Set creation by accessing all the files from the drive path\n",
        "FeatureSet = set()\n",
        "\n",
        "EmailMap = defaultdict(set)\n",
        "EmailMapTF = defaultdict(list)\n",
        "\n",
        "TotalFolders = 10\n",
        "\n",
        "# Run through all the files in the folder\n",
        "for pathnum in range(1, TotalFolders+1):\n",
        "  Subfolder =dirpath + f\"/part{pathnum}\" \n",
        "  all_entries = scandir(Subfolder)\n",
        "\n",
        "# For an entry in the all files \n",
        "  for entry in all_entries:\n",
        "    Filepath = Subfolder + f\"/{entry.name}\"\n",
        "\n",
        "# Readind a file \n",
        "    with open(Filepath, 'r') as thisfile:\n",
        "      data = thisfile.read()\n",
        "      # set for Bernoulli and Multinomial\n",
        "      FileFeatureset = set(data.split())\n",
        "      # List output for Term Frequency Calculations\n",
        "      EmailMapTFfeatureMap = list(data.split())\n",
        "      if Filepath not in EmailMap:\n",
        "        EmailMap[str(Filepath)]= FileFeatureset\n",
        "\n",
        "      if Filepath not in EmailMapTF:\n",
        "        EmailMapTF[str(Filepath)] = EmailMapTFfeatureMap\n",
        "        #print(EmailMap)\n",
        "      FeatureSet = FeatureSet.union(FileFeatureset)      # Combing Feature Set\n",
        "\n",
        "print(len(EmailMap))\n",
        "print(FeatureSet)\n",
        "#FeatureSet = FeatureSet.difference(set(Cleanerlist))\n",
        "print(len(FeatureSet))\n",
        "#print(EmailMap)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYeAqrbQwMVR"
      },
      "source": [
        "Generating 2 seperate DataFrame Objects for Term Frequency (TF) and Binary DataFrame Object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1N1sP5Qu3xkO"
      },
      "source": [
        "import numpy as np\n",
        "import csv \n",
        "\n",
        "# The FeatureSet is added 1 to Add SPAM Column\n",
        "LastColumn = len(FeatureSet)\n",
        "zeros_array = np.zeros( (len(EmailMap), LastColumn+1))\n",
        "TF_arry = np.zeros( (len(EmailMap), LastColumn+1))\n",
        "# Create a Map to zip in the index of the word in the FeatureSet\n",
        "IndexMapofFeatureSet = {}\n",
        "start = 0\n",
        "for v in FeatureSet:\n",
        "  if v not in IndexMapofFeatureSet:\n",
        "    IndexMapofFeatureSet[v] = start\n",
        "    start += 1\n",
        "\n",
        "# Printing to check \n",
        "#print(IndexMapofFeatureSet)\n",
        "\n",
        "# To fill the matrix of Emails x Words \n",
        "# Here 1 represents the presence of Words and \n",
        "# 0 represents absence\n",
        "Row = 0\n",
        "for K,V_Set in EmailMap.items():\n",
        "  for Val in V_Set:\n",
        "    zeros_array[Row][IndexMapofFeatureSet[str(Val)]] = 1\n",
        "  if \"spm\" in K:\n",
        "    zeros_array[Row][LastColumn] = 1\n",
        "  Row += 1\n",
        "\n",
        "\n",
        "# For Term Frequency \n",
        "\n",
        "Row = 0\n",
        "for K,V_Set in EmailMapTF.items():\n",
        "  for Val in V_Set:\n",
        "    #if str(Val) not in Cleanerlist:\n",
        "    TF_arry[Row][IndexMapofFeatureSet[str(Val)]] += 1\n",
        "  if \"spm\" in K:\n",
        "    TF_arry[Row][LastColumn] = 1\n",
        "  Row += 1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEeVYttw16Hz"
      },
      "source": [
        "# List of FeatureSet\n",
        "FeaturesList = list(FeatureSet)\n",
        "\n",
        "FeaturesList.append('SPAM')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clHuxCWgwjn8"
      },
      "source": [
        "Adding DataFrame Objects to CSV File so that it is easier to Load next Time (Both Dataframes TF as well as Binary)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvV_BpBU5R_I"
      },
      "source": [
        "FilepathTFDataSet = '/content/drive/My Drive/Consolidated_TF_EmailCSV.csv'\n",
        "\n",
        "# writing to csv file  \n",
        "with open(FilepathTFDataSet, 'w') as csvfile:  \n",
        "    # creating a csv writer object  \n",
        "    csvwriter = csv.writer(csvfile)  \n",
        "        \n",
        "    # writing the fields  \n",
        "    csvwriter.writerow(FeaturesList)  \n",
        "        \n",
        "    # writing the data rows  \n",
        "    csvwriter.writerows(TF_arry) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N7xbWkr2JU6"
      },
      "source": [
        "FilePath = '/content/drive/My Drive/ConsolidatedEmailCSV.csv'\n",
        "\n",
        "# writing to csv file  \n",
        "with open(FilePath, 'w') as csvfile:  \n",
        "    # creating a csv writer object  \n",
        "    csvwriter = csv.writer(csvfile)  \n",
        "        \n",
        "    # writing the fields  \n",
        "    csvwriter.writerow(FeaturesList)  \n",
        "        \n",
        "    # writing the data rows  \n",
        "    csvwriter.writerows(zeros_array) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3R091ChwxRF"
      },
      "source": [
        "Reading TF Dataset from CSV file. Makes it lot more easier to load data than process it everytime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJLgo4zU5VlA",
        "outputId": "96ab73f2-d650-4c45-e08d-5b38e41fd709",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "FilePath = '/content/drive/My Drive/Consolidated_TF_EmailCSV.csv'\n",
        "\n",
        "TF_DataSet = pd.read_csv(FilePath)\n",
        "\n",
        "TF_DataSet.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>136</th>\n",
              "      <th>heathrow</th>\n",
              "      <th>1100-1800</th>\n",
              "      <th>embryogense</th>\n",
              "      <th>40351</th>\n",
              "      <th>corey</th>\n",
              "      <th>observer</th>\n",
              "      <th>odonnaile</th>\n",
              "      <th>808-942</th>\n",
              "      <th>86d</th>\n",
              "      <th>fortunecity</th>\n",
              "      <th>tient</th>\n",
              "      <th>160-171</th>\n",
              "      <th>wiren</th>\n",
              "      <th>romajus</th>\n",
              "      <th>gamer</th>\n",
              "      <th>anhalt</th>\n",
              "      <th>pragmaphilological</th>\n",
              "      <th>savickiene</th>\n",
              "      <th>igy</th>\n",
              "      <th>310-823</th>\n",
              "      <th>junk</th>\n",
              "      <th>arnaiz</th>\n",
              "      <th>search-and</th>\n",
              "      <th>11391</th>\n",
              "      <th>bellus</th>\n",
              "      <th>salvat</th>\n",
              "      <th>especial</th>\n",
              "      <th>eckhardt</th>\n",
              "      <th>hiraus</th>\n",
              "      <th>restaurant</th>\n",
              "      <th>jay</th>\n",
              "      <th>stately</th>\n",
              "      <th>erosion</th>\n",
              "      <th>90095-1543</th>\n",
              "      <th>computer-base</th>\n",
              "      <th>elliston</th>\n",
              "      <th>lennard</th>\n",
              "      <th>verkehrssprachen</th>\n",
              "      <th>range</th>\n",
              "      <th>...</th>\n",
              "      <th>ciao</th>\n",
              "      <th>aghast</th>\n",
              "      <th>type-set</th>\n",
              "      <th>20846</th>\n",
              "      <th>732</th>\n",
              "      <th>atholic</th>\n",
              "      <th>orientalium</th>\n",
              "      <th>finnischen</th>\n",
              "      <th>maddieson</th>\n",
              "      <th>videophone</th>\n",
              "      <th>94720-2580</th>\n",
              "      <th>calvus</th>\n",
              "      <th>225</th>\n",
              "      <th>harangue</th>\n",
              "      <th>kozonoj</th>\n",
              "      <th>0855-22</th>\n",
              "      <th>gelso</th>\n",
              "      <th>iphi</th>\n",
              "      <th>11733</th>\n",
              "      <th>grants</th>\n",
              "      <th>eigenen</th>\n",
              "      <th>transducers</th>\n",
              "      <th>biterly</th>\n",
              "      <th>copies</th>\n",
              "      <th>palmada</th>\n",
              "      <th>387-427</th>\n",
              "      <th>yuha</th>\n",
              "      <th>deviate</th>\n",
              "      <th>cleanest</th>\n",
              "      <th>cracovie</th>\n",
              "      <th>base64</th>\n",
              "      <th>introducao</th>\n",
              "      <th>raleigh</th>\n",
              "      <th>msus</th>\n",
              "      <th>telo</th>\n",
              "      <th>wante</th>\n",
              "      <th>multi-tap</th>\n",
              "      <th>intellect</th>\n",
              "      <th>scarfield</th>\n",
              "      <th>SPAM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 59736 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   136  heathrow  1100-1800  embryogense  ...  multi-tap  intellect  scarfield  SPAM\n",
              "0  0.0       0.0        0.0          0.0  ...        0.0        0.0        0.0   0.0\n",
              "1  0.0       0.0        0.0          0.0  ...        0.0        0.0        0.0   0.0\n",
              "2  0.0       0.0        0.0          0.0  ...        0.0        0.0        0.0   0.0\n",
              "3  0.0       0.0        0.0          0.0  ...        0.0        0.0        0.0   0.0\n",
              "4  0.0       0.0        0.0          0.0  ...        0.0        0.0        0.0   0.0\n",
              "\n",
              "[5 rows x 59736 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ5GUDtQw72B"
      },
      "source": [
        "Load Binary Dataset from CSV File."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYPGHhuK3TBm",
        "outputId": "75eeda0c-57fa-4ffd-9370-4a49572e866e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "FilePath = '/content/drive/My Drive/ConsolidatedEmailCSV.csv'\n",
        "\n",
        "DataSet = pd.read_csv(FilePath)\n",
        "\n",
        "DataSet.head()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>136</th>\n",
              "      <th>heathrow</th>\n",
              "      <th>1100-1800</th>\n",
              "      <th>embryogense</th>\n",
              "      <th>40351</th>\n",
              "      <th>corey</th>\n",
              "      <th>observer</th>\n",
              "      <th>odonnaile</th>\n",
              "      <th>808-942</th>\n",
              "      <th>86d</th>\n",
              "      <th>fortunecity</th>\n",
              "      <th>tient</th>\n",
              "      <th>160-171</th>\n",
              "      <th>wiren</th>\n",
              "      <th>romajus</th>\n",
              "      <th>gamer</th>\n",
              "      <th>anhalt</th>\n",
              "      <th>pragmaphilological</th>\n",
              "      <th>savickiene</th>\n",
              "      <th>igy</th>\n",
              "      <th>310-823</th>\n",
              "      <th>junk</th>\n",
              "      <th>arnaiz</th>\n",
              "      <th>search-and</th>\n",
              "      <th>11391</th>\n",
              "      <th>bellus</th>\n",
              "      <th>salvat</th>\n",
              "      <th>especial</th>\n",
              "      <th>eckhardt</th>\n",
              "      <th>hiraus</th>\n",
              "      <th>restaurant</th>\n",
              "      <th>jay</th>\n",
              "      <th>stately</th>\n",
              "      <th>erosion</th>\n",
              "      <th>90095-1543</th>\n",
              "      <th>computer-base</th>\n",
              "      <th>elliston</th>\n",
              "      <th>lennard</th>\n",
              "      <th>verkehrssprachen</th>\n",
              "      <th>range</th>\n",
              "      <th>...</th>\n",
              "      <th>ciao</th>\n",
              "      <th>aghast</th>\n",
              "      <th>type-set</th>\n",
              "      <th>20846</th>\n",
              "      <th>732</th>\n",
              "      <th>atholic</th>\n",
              "      <th>orientalium</th>\n",
              "      <th>finnischen</th>\n",
              "      <th>maddieson</th>\n",
              "      <th>videophone</th>\n",
              "      <th>94720-2580</th>\n",
              "      <th>calvus</th>\n",
              "      <th>225</th>\n",
              "      <th>harangue</th>\n",
              "      <th>kozonoj</th>\n",
              "      <th>0855-22</th>\n",
              "      <th>gelso</th>\n",
              "      <th>iphi</th>\n",
              "      <th>11733</th>\n",
              "      <th>grants</th>\n",
              "      <th>eigenen</th>\n",
              "      <th>transducers</th>\n",
              "      <th>biterly</th>\n",
              "      <th>copies</th>\n",
              "      <th>palmada</th>\n",
              "      <th>387-427</th>\n",
              "      <th>yuha</th>\n",
              "      <th>deviate</th>\n",
              "      <th>cleanest</th>\n",
              "      <th>cracovie</th>\n",
              "      <th>base64</th>\n",
              "      <th>introducao</th>\n",
              "      <th>raleigh</th>\n",
              "      <th>msus</th>\n",
              "      <th>telo</th>\n",
              "      <th>wante</th>\n",
              "      <th>multi-tap</th>\n",
              "      <th>intellect</th>\n",
              "      <th>scarfield</th>\n",
              "      <th>SPAM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 59736 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   136  heathrow  1100-1800  embryogense  ...  multi-tap  intellect  scarfield  SPAM\n",
              "0  0.0       0.0        0.0          0.0  ...        0.0        0.0        0.0   0.0\n",
              "1  0.0       0.0        0.0          0.0  ...        0.0        0.0        0.0   0.0\n",
              "2  0.0       0.0        0.0          0.0  ...        0.0        0.0        0.0   0.0\n",
              "3  0.0       0.0        0.0          0.0  ...        0.0        0.0        0.0   0.0\n",
              "4  0.0       0.0        0.0          0.0  ...        0.0        0.0        0.0   0.0\n",
              "\n",
              "[5 rows x 59736 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttlDetc_2nap"
      },
      "source": [
        "Information Gain Calculations are carried out Below.\n",
        "\n",
        "*   The Information Gain is also later written to the file for easy of loading next time when I work with it. \n",
        "*   Occurance is considered not Frequency.\n",
        "*   (Takes a lot of time to calculate, use already written CSV)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ih1k_zdS0CUc",
        "outputId": "5d1f5e8c-9d45-4dc0-a421-ae4852372d12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        " \n",
        "import math\n",
        "\n",
        "DataSetLen = len(DataSet[\"SPAM\"])\n",
        "print(\"DataSetLen\",DataSetLen)\n",
        "SpamArry = [0] * DataSetLen\n",
        "SpamArry = [int(sub) for sub in DataSet[\"SPAM\"]] \n",
        "\n",
        "count = 0\n",
        "\n",
        "for i in SpamArry:\n",
        "  if i == 1:\n",
        "    count+=1\n",
        "Legitp = float((DataSetLen - count)/DataSetLen)\n",
        "Legit_Count = (DataSetLen - count)\n",
        "print(\"Legit_Count \",Legit_Count)\n",
        "Spam_Count = count\n",
        "print(\"Spam_Count \", Spam_Count)\n",
        "#print(Legitp)\n",
        "spamp = float(count/DataSetLen)\n",
        "#print(spamp)\n",
        "p1  = -spamp*math.log(spamp,10)\n",
        "p2  = -(1-spamp)*math.log((1-spamp),10)\n",
        "H_of_C = p1 + p2\n",
        "#print(p1+p2)\n",
        "\n",
        "Present_given_spam = 0\n",
        "Not_Present_given_spam = 0\n",
        "Present_given_Ham = 0\n",
        "Not_Present_given_Ham = 0\n",
        "Present = 0\n",
        "Not_Present = 0\n",
        "TotalEmails = DataSetLen\n",
        "Word_IG_dict = {}\n",
        "#print(DataSet[\"\"])\n",
        "for i in DataSet:\n",
        "  if i != \"SPAM\":\n",
        "    for j,k in enumerate(DataSet[i]):\n",
        "      if int(k) == 1:\n",
        "        Present+=1\n",
        "        if int(DataSet[\"SPAM\"][j]) == 1:\n",
        "          Present_given_spam += 1\n",
        "          #print(\"Present_given_spam: \", Present_given_spam)\n",
        "        else:\n",
        "          Present_given_Ham += 1\n",
        "          #print(\"Present_given_Ham: \", Present_given_Ham)\n",
        "      elif int(k) == 0:\n",
        "        Not_Present+=1\n",
        "        if int(DataSet[\"SPAM\"][j]) == 1:\n",
        "          Not_Present_given_spam += 1\n",
        "          #print(\"Not_Present_given_spam: \", Not_Present_given_spam)\n",
        "        else:\n",
        "          Not_Present_given_Ham += 1\n",
        "          #print(\"Not_Present_given_Ham: \", Not_Present_given_Ham)\n",
        "    \n",
        "    \n",
        "    if Not_Present!=0:\n",
        "      P_Present = float(Present/TotalEmails)\n",
        "      P_Not_Present = float(Not_Present/TotalEmails)\n",
        "\n",
        "      P_Present_given_Spam = float((Present_given_spam+1)/(Spam_Count+2))\n",
        "      Joint_Prob_Present_Spam = P_Present_given_Spam * spamp \n",
        "      Prob_Spam_given_Present = math.log(P_Present_given_Spam * spamp / P_Present,10)\n",
        "\n",
        "\n",
        "      P_Not_Present_given_Spam = float((Not_Present_given_spam+1)/(Spam_Count+2))\n",
        "      Joint_Prob_Not_Present_Spam = P_Not_Present_given_Spam * spamp \n",
        "      Prob_Spam_given_Not_Present = math.log(P_Not_Present_given_Spam * spamp / P_Not_Present,10)\n",
        "            \n",
        "\n",
        "      P_Present_given_Ham = float((Present_given_Ham+1)/(Legit_Count+2))      \n",
        "      Joint_Prob_Present_Ham = P_Present_given_Ham * Legitp \n",
        "      Prob_Ham_given_Present = math.log(P_Present_given_Ham * Legitp /P_Present,10)\n",
        "\n",
        "\n",
        "      P_Not_Present_given_Ham = float((Not_Present_given_Ham+1)/(Legit_Count+2))\n",
        "      Joint_Prob_Not_Present_Ham = P_Not_Present_given_Ham * Legitp \n",
        "      Prob_Ham_given_Not_Present = math.log(P_Not_Present_given_Ham * Legitp/P_Not_Present,10)\n",
        "          \n",
        "      H_of_C_given_X = (-1.0)* (Joint_Prob_Present_Spam*Prob_Spam_given_Present + Joint_Prob_Not_Present_Spam*Prob_Spam_given_Not_Present + Joint_Prob_Present_Ham*Prob_Ham_given_Present + Joint_Prob_Not_Present_Ham*Prob_Ham_given_Not_Present)\n",
        "\n",
        "      IG = H_of_C - H_of_C_given_X\n",
        "      if str(i) not in Word_IG_dict:\n",
        "        Word_IG_dict[i] = IG\n",
        "    else:\n",
        "      print(\"0 Gain feature: \", i)\n",
        "    #first_term = Joint_Prob_Present_Spam * Prob_Spam_given_Present\n",
        "\n",
        "\n",
        "    Present_given_spam = 0\n",
        "    Not_Present_given_spam = 0\n",
        "    Present_given_Ham = 0\n",
        "    Not_Present_given_Ham = 0\n",
        "    Present = 0\n",
        "    Not_Present = 0\n",
        "    #print(first_term)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DataSetLen 2893\n",
            "Legit_Count  2412\n",
            "Spam_Count  481\n",
            "0 Gain feature:  Subject:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAx6cBZyMq0B"
      },
      "source": [
        "Information Gain Is written to CSV for fasters processing \n",
        "(Developer Note >> USE ONCE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4rmfRu4jIH3"
      },
      "source": [
        "import csv \n",
        "FilePathIGmap = '/content/drive/My Drive/IGmap.csv'\n",
        "\n",
        "# writing to csv file  \n",
        "with open(FilePathIGmap, 'w') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)    \n",
        "        \n",
        "    # writing the data rows  \n",
        "    csvwriter.writerows(Word_IG_dict.items()) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ1DNQXN6p8w"
      },
      "source": [
        " Read IG from CSV                    (Developer Note >>>  Start Here) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSYoWl-Il05S"
      },
      "source": [
        "import csv \n",
        "FilePathIGmap = '/content/drive/My Drive/IGmap.csv'\n",
        "fetchedIG = {}\n",
        "\n",
        "with open(FilePathIGmap) as csvfile:\n",
        "    \n",
        "    readCSV = csv.reader(csvfile, delimiter=',')\n",
        "    \n",
        "    for row in readCSV:\n",
        "      fetchedIG[row[0]] = float(row[1])\n",
        "\n",
        "#print(fetchedIG)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4orT7XuM9-h"
      },
      "source": [
        "## Top 10 Features Printed Below\n",
        "Getting Top N Features ( N = 10, 100, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnsmB5bwO8WM",
        "outputId": "577746ba-e891-4b99-ab7c-8fd5afb1f005",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "Sorted_IG_dict={ k:v for k,v in sorted(fetchedIG.items(), key = lambda x: x[1], reverse=True) }\n",
        "# print(Sorted_IG_dict[:11])\n",
        "Num = 1\n",
        "FCombLen = 3\n",
        "Top10features = []\n",
        "Top100Features = []\n",
        "Top1000Features = []\n",
        "VarFeatures = [[] for i in range(FCombLen)]\n",
        "#print(len(VarFeatures))\n",
        "for Key in Sorted_IG_dict:\n",
        "    #print( Val, \" \", fetchedIG[Val])\n",
        "    #TopNfeatures.append(Key) \n",
        "    if Num <= 10:\n",
        "      Top10features.append(Key) \n",
        "      if Num == 10:\n",
        "        VarFeatures[0] = Top10features  \n",
        "    if Num <= 100:\n",
        "      Top100Features.append(Key)\n",
        "      if Num == 100:\n",
        "        VarFeatures[1] = Top100Features\n",
        "    if Num <= 1000:\n",
        "      Top1000Features.append(Key)\n",
        "      if Num == 1000: \n",
        "        VarFeatures[2] = Top1000Features\n",
        "        break\n",
        "    Num+=1\n",
        "\n",
        "#print(VarFeatures)\n",
        "# for i in VarFeatures:\n",
        "#   print(len(i))\n",
        "\n",
        "print(\"Top 10 Features: \")\n",
        "print(Top10features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 10 Features: \n",
            "['!', 'language', 'remove', 'free', 'university', 'linguistic', 'money', 'click', '@', 'our']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1HrjVjUNxI3"
      },
      "source": [
        "Calculating Feature Instances in Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnSAX__9cmvm",
        "outputId": "20c62a13-b25e-465a-c552-ff338c4e7df9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Part 1 to - Part 9 training \n",
        "\n",
        "#part 10 total files\n",
        "filenum = 291\n",
        "\n",
        "map_Feature_Given_Spam = {}\n",
        "map_Feature_Given_Ham = {}\n",
        " \n",
        "DataSetLen = len(DataSet[\"SPAM\"])\n",
        "print(\"DataSetLen\",DataSetLen)\n",
        "SpamArry = [0] * DataSetLen\n",
        "SpamArry = [int(sub) for sub in DataSet[\"SPAM\"]] \n",
        "\n",
        "count = 0\n",
        "\n",
        "for i in SpamArry:\n",
        "  if i == 1:\n",
        "    count+=1\n",
        "\n",
        "Legitp = float((DataSetLen - count)/DataSetLen)\n",
        "Legit_Count = (DataSetLen - count)\n",
        "print(\"Legit_Count \",Legit_Count)\n",
        "Spam_Count = count\n",
        "print(\"Spam_Count \", Spam_Count)\n",
        "#print(Legitp)\n",
        "spamp = float(count/DataSetLen)\n",
        "#print(spamp)\n",
        "\n",
        "# Selecting the highest Feature list\n",
        "for k in Top1000Features:\n",
        "  map_Feature_Given_Spam[k] = 0\n",
        "  map_Feature_Given_Ham[k] = 0\n",
        "\n",
        "#Map of Features for 1 to 9 parts\n",
        "for i in DataSet:\n",
        "  if i != \"SPAM\" and i in Top1000Features:\n",
        "    for j in range(0,(len(DataSet[i])-filenum)):\n",
        "      if int(DataSet[i][j]) == 1 and int(DataSet[\"SPAM\"][j]) == 1:\n",
        "        map_Feature_Given_Spam[i]+=1\n",
        "      elif int(DataSet[i][j]) == 1 and int(DataSet[\"SPAM\"][j]) == 0:\n",
        "        map_Feature_Given_Ham[i]+=1\n",
        "\n",
        "\n",
        "\n",
        "# Spam_Count and Legit_Count - part10 outputs \n",
        "TotalEmails = len(SpamArry)\n",
        "SpamCnt1to9prt = Spam_Count\n",
        "HamCnt1to9prt = Legit_Count\n",
        "TotalTest = 0\n",
        "for i in range(TotalEmails-filenum,TotalEmails):\n",
        "  TotalTest +=1\n",
        "  if SpamArry[i] == 0:\n",
        "    HamCnt1to9prt -= 1\n",
        "  else:\n",
        "    SpamCnt1to9prt -= 1 \n",
        "\n",
        "print(\"Total Spam_Count\",Spam_Count)\n",
        "print(\"Total Legit_Count\",Legit_Count)\n",
        "print(\"Total Test Files\",TotalTest)\n",
        "print(\"Spam Count in 1 to 9 parts: \",SpamCnt1to9prt)\n",
        "print(\"Ham Count in 1 to 9 parts: \",HamCnt1to9prt)\n",
        "# print(\"Feature Given Ham: \")\n",
        "# print(map_Feature_Given_Ham)\n",
        "# print(\"Feature Given Spam\")\n",
        "# print(map_Feature_Given_Spam)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DataSetLen 2893\n",
            "Legit_Count  2412\n",
            "Spam_Count  481\n",
            "Total Spam_Count 481\n",
            "Total Legit_Count 2412\n",
            "Total Test Files 291\n",
            "Spam Count in 1 to 9 parts:  432\n",
            "Ham Count in 1 to 9 parts:  2170\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPcHB_uSwZ86"
      },
      "source": [
        "Feature Given Spam and Feature Given Legit /Ham Probability Calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tu6UxswWYOQ"
      },
      "source": [
        "map_Prob_Feature_Given_Ham = {}\n",
        "map_Prob_Feature_Given_Spam = {k:float((v+1)/(SpamCnt1to9prt+2)) for k,v in map_Feature_Given_Spam.items()}\n",
        "for Key,Val in map_Feature_Given_Ham.items():\n",
        "  map_Prob_Feature_Given_Ham[Key] = float((Val+1)/(HamCnt1to9prt+2))\n",
        "\n",
        "print(\"Probability of Feature Given Ham Map Below:\")\n",
        "print(map_Prob_Feature_Given_Ham)\n",
        "print(\"Prbability of Feature Given Spam Map Below:\")\n",
        "print(map_Prob_Feature_Given_Spam)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWqTN64V9rcZ"
      },
      "source": [
        "Calculating Feature Instances in TF Dataset of Term Frequencies (TF)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOgqIVio9qZN"
      },
      "source": [
        "#part 10 total files\n",
        "filenum = 291\n",
        "\n",
        "map_TF_Feature_Given_Spam = {}\n",
        "map_TF_Feature_Given_Ham = {}\n",
        " \n",
        "# TF_DataSetLen = len(DataSetTF[\"SPAM\"])\n",
        "# print(\"DataSetLen\",TF_DataSetLen)\n",
        "# SpamArry = [0] * TF_DataSetLen\n",
        "# SpamArry = [int(sub) for sub in DataSetTF[\"SPAM\"]] \n",
        "\n",
        "# Selecting the highest Feature list\n",
        "for k in Top1000Features:\n",
        "  map_TF_Feature_Given_Spam[k] = 0\n",
        "  map_TF_Feature_Given_Ham[k] = 0\n",
        "\n",
        "#Map of Features for 1 to 9 parts\n",
        "for i in TF_DataSet:\n",
        "  if i != \"SPAM\" and i in Top1000Features:\n",
        "    for j in range(0,(len(TF_DataSet[i])-filenum)):\n",
        "      if int(TF_DataSet[i][j]) > 0 and int(TF_DataSet[\"SPAM\"][j]) == 1:\n",
        "        map_TF_Feature_Given_Spam[i]+= int(TF_DataSet[i][j])\n",
        "      elif int(TF_DataSet[i][j]) > 0 and int(TF_DataSet[\"SPAM\"][j]) == 0:\n",
        "        map_TF_Feature_Given_Ham[i]+=int(TF_DataSet[i][j])\n",
        "\n",
        "print(\"TF Feature Given Ham: \")\n",
        "print(map_TF_Feature_Given_Ham)\n",
        "print(\"TF Feature Given Spam: \")\n",
        "print(map_TF_Feature_Given_Spam)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1bloPvSNlMh"
      },
      "source": [
        "Write TF Feature Presence Count in Ham and Spam for easy retrival"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUmY9NlPNkPv"
      },
      "source": [
        "import csv \n",
        "FilePathFeatureGSpamTF = '/content/drive/My Drive/map_TF_Feature_Given_Spam.csv'\n",
        "FilePathFeatureGHamTF = '/content/drive/My Drive/map_TF_Feature_Given_Ham.csv'\n",
        "\n",
        "# writing to csv file  \n",
        "with open(FilePathFeatureGHamTF, 'w') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)    \n",
        "        \n",
        "    # writing the data rows  \n",
        "    csvwriter.writerows(map_TF_Feature_Given_Ham.items()) \n",
        "\n",
        "# writing to csv file  \n",
        "with open(FilePathFeatureGSpamTF, 'w') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)    \n",
        "        \n",
        "    # writing the data rows  \n",
        "    csvwriter.writerows(map_TF_Feature_Given_Spam.items()) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziq9XU99REPp"
      },
      "source": [
        "Read TF Feature Count from CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKtiD1tDRBcG"
      },
      "source": [
        "import csv \n",
        "fetchedIG = {}\n",
        "FilePathFeatureGSpamTF = '/content/drive/My Drive/map_TF_Feature_Given_Spam.csv'\n",
        "FilePathFeatureGHamTF = '/content/drive/My Drive/map_TF_Feature_Given_Ham.csv'\n",
        "map_TF_Feature_Given_Ham = {}\n",
        "map_TF_Feature_Given_Spam = {}\n",
        "with open(FilePathFeatureGHamTF) as csvfile:\n",
        "    \n",
        "    readCSV = csv.reader(csvfile, delimiter=',')\n",
        "    \n",
        "    for row in readCSV:\n",
        "      map_TF_Feature_Given_Ham[row[0]] = float(row[1])\n",
        "\n",
        "with open(FilePathFeatureGSpamTF) as csvfile:\n",
        "    \n",
        "    readCSV = csv.reader(csvfile, delimiter=',')\n",
        "    \n",
        "    for row in readCSV:\n",
        "      map_TF_Feature_Given_Spam[row[0]] = float(row[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2-t0kHpni-t"
      },
      "source": [
        "Total Words in Spam and Total Words in Ham calculated below!  - (Developer Note >>> Takes a Lot of Time, run once)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5uz2rrHfCnH",
        "outputId": "0891e8e0-fc80-4144-d059-4c0dcc564c71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "Total_Words_Spam = 0\n",
        "Total_Words_Ham = 0\n",
        "for i in TF_DataSet:\n",
        "  for j in range(0,len(TF_DataSet[i])-filenum):\n",
        "    if int(TF_DataSet[i][j]) > 0 and int(TF_DataSet[\"SPAM\"][j]) == 1:\n",
        "        Total_Words_Spam += int(TF_DataSet[i][j])\n",
        "    elif int(TF_DataSet[i][j]) > 0 and int(TF_DataSet[\"SPAM\"][j]) == 0:\n",
        "        Total_Words_Ham+=int(TF_DataSet[i][j])\n",
        "print(Total_Words_Spam)\n",
        "print(Total_Words_Ham)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "297052\n",
            "1000879\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEVTYUCAoRFk"
      },
      "source": [
        "Calculating Feature Probabilty for TF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuvcUk6Vn13o"
      },
      "source": [
        "# Use for faster calculations\n",
        "# Total Spam word - 250513\n",
        "# Total Ham words - 785662\n",
        "# size of Laplasian Denominator \n",
        "Total_Words_Spam = 297052\n",
        "Total_Words_Ham = 1000879\n",
        "\n",
        "\n",
        "#M = len(map_Feature_Given_Spam)\n",
        "#print(\"M :\", M)\n",
        "# For M = 1000\n",
        "map_TF_Prob_Feature_Given_Ham = {k:float((v+1)/(Total_Words_Ham+1000)) for k,v in map_TF_Feature_Given_Ham.items()}\n",
        "map_TF_Prob_Feature_Given_Spam = {k:float((v+1)/(Total_Words_Spam+1000)) for k,v in map_TF_Feature_Given_Spam.items()}\n",
        "\n",
        "# For M = 100\n",
        "map_TF_M100_Prob_Feature_Given_Ham = {k:float((v+1)/(Total_Words_Ham+100)) for k,v in map_TF_Feature_Given_Ham.items()}\n",
        "map_TF_M100_Prob_Feature_Given_Spam = {k:float((v+1)/(Total_Words_Spam+100)) for k,v in map_TF_Feature_Given_Spam.items()}\n",
        "\n",
        "# For M = 10\n",
        "map_TF_M10_Prob_Feature_Given_Ham = {k:float((v+1)/(Total_Words_Ham+10)) for k,v in map_TF_Feature_Given_Ham.items()}\n",
        "map_TF_M10_Prob_Feature_Given_Spam = {k:float((v+1)/(Total_Words_Spam+10)) for k,v in map_TF_Feature_Given_Spam.items()}\n",
        "\n",
        "#print(map_TF_Prob_Feature_Given_Ham)\n",
        "#print(map_TF_Prob_Feature_Given_Spam)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzWGigCYguX_"
      },
      "source": [
        "Write Feature Probability so its easier to load when needed! B-NB\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38Qw93Ta7PiG"
      },
      "source": [
        "import csv \n",
        "FilePathFeatureGHam = '/content/drive/My Drive/map_Prob_Feature_Given_Ham.csv'\n",
        "FilePathFeatureGSpam = '/content/drive/My Drive/map_Prob_Feature_Given_Spam.csv'\n",
        "\n",
        "# writing to csv file  \n",
        "with open(FilePathFeatureGHam, 'w') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)    \n",
        "        \n",
        "    # writing the data rows  \n",
        "    csvwriter.writerows(map_Prob_Feature_Given_Ham.items()) \n",
        "\n",
        "# writing to csv file  \n",
        "with open(FilePathFeatureGSpam, 'w') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)    \n",
        "        \n",
        "    # writing the data rows  \n",
        "    csvwriter.writerows(map_Prob_Feature_Given_Spam.items()) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T75_K_qug2eA"
      },
      "source": [
        "Read Feature Probability given ham/spam for Models B-NB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8uI__ZW84lG"
      },
      "source": [
        "import csv \n",
        "fetchedIG = {}\n",
        "FilePathFeatureGHam = '/content/drive/My Drive/map_Prob_Feature_Given_Ham.csv'\n",
        "FilePathFeatureGSpam = '/content/drive/My Drive/map_Prob_Feature_Given_Spam.csv'\n",
        "map_Prob_Feature_Given_Ham = {}\n",
        "map_Prob_Feature_Given_Spam = {}\n",
        "with open(FilePathFeatureGHam) as csvfile:\n",
        "    \n",
        "    readCSV = csv.reader(csvfile, delimiter=',')\n",
        "    \n",
        "    for row in readCSV:\n",
        "      map_Prob_Feature_Given_Ham[row[0]] = float(row[1])\n",
        "\n",
        "with open(FilePathFeatureGSpam) as csvfile:\n",
        "    \n",
        "    readCSV = csv.reader(csvfile, delimiter=',')\n",
        "    \n",
        "    for row in readCSV:\n",
        "      map_Prob_Feature_Given_Spam[row[0]] = float(row[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9mXQytmlDyw"
      },
      "source": [
        "## All Classifier Precision, Recall, accuracy, confusion Matrix output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC-b1oFkjMp8",
        "outputId": "77fcd8a7-7845-414e-b809-505dc3d0609d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "Pred_Ouput = []\n",
        "Spam_Pred = []\n",
        "Ham_Pred = []\n",
        "\n",
        "MN_Spam_Pred = []\n",
        "MN_HamPred = []\n",
        "MN_Pred_Ouput = []\n",
        "\n",
        "Prob_Spam1to9 = float(SpamCnt1to9prt/(SpamCnt1to9prt+HamCnt1to9prt))\n",
        "Prob_Ham1to9 = float(HamCnt1to9prt/(SpamCnt1to9prt+HamCnt1to9prt))\n",
        "\n",
        "ActualOutput = [SpamArry[i] for i in range(TotalEmails-filenum, TotalEmails)]\n",
        "len(ActualOutput)\n",
        "for TopNfeatures in VarFeatures:\n",
        "  Pred_Ouput = []\n",
        "  Spam_Pred = []\n",
        "  Ham_Pred = []\n",
        "\n",
        "  MN_Spam_Pred = []\n",
        "  MN_HamPred = []\n",
        "  MN_Pred_Ouput = []\n",
        "  \n",
        "  TF_Pred_Output = []\n",
        "  TF_Ham_Pred = []\n",
        "  TF_Spam_Pred = []\n",
        "  for j in range(TotalEmails-filenum, TotalEmails):\n",
        "    FeatureBits = {}\n",
        "    for i in DataSet:\n",
        "      if i !=\"SPAM\" and i in TopNfeatures:\n",
        "        if int(DataSet[i][j]) == 1:\n",
        "          FeatureBits[i] = 1   \n",
        "        else:\n",
        "          FeatureBits[i] = 0\n",
        "    #print(FeatureBits)\n",
        "    SpamPredProb_var = 1.0*Prob_Spam1to9\n",
        "    HamPredProb_var = 1.0*Prob_Ham1to9\n",
        "\n",
        "    MN_SpamPredProb_var = 1.0*Prob_Spam1to9\n",
        "    MN_HamPredProb_var = 1.0*Prob_Ham1to9\n",
        "\n",
        "    TF_SpamPredProb_var = 1.0*Prob_Spam1to9\n",
        "    TF_HamPredProb_var = 1.0*Prob_Ham1to9\n",
        "    \n",
        "    for K,V in FeatureBits.items():\n",
        "      if V == 1:\n",
        "        SpamPredProb_var *= map_Prob_Feature_Given_Spam[K]\n",
        "        HamPredProb_var *= map_Prob_Feature_Given_Ham[K]\n",
        "        MN_SpamPredProb_var *= map_Prob_Feature_Given_Spam[K]\n",
        "        MN_HamPredProb_var *= map_Prob_Feature_Given_Ham[K]\n",
        "\n",
        "        if len(TopNfeatures) == 1000:\n",
        "           TF_SpamPredProb_var *= map_TF_Prob_Feature_Given_Spam[K]**TF_DataSet[K][j]\n",
        "           TF_HamPredProb_var *= map_TF_Prob_Feature_Given_Ham[K]**TF_DataSet[K][j]\n",
        "        if len(TopNfeatures) == 100:\n",
        "           TF_SpamPredProb_var *= map_TF_M100_Prob_Feature_Given_Spam[K]**TF_DataSet[K][j]\n",
        "           TF_HamPredProb_var *= map_TF_M100_Prob_Feature_Given_Ham[K]**TF_DataSet[K][j]\n",
        "        if len(TopNfeatures) == 10:\n",
        "           TF_SpamPredProb_var *= map_TF_M10_Prob_Feature_Given_Spam[K]**TF_DataSet[K][j]\n",
        "           TF_HamPredProb_var *= map_TF_M10_Prob_Feature_Given_Ham[K]**TF_DataSet[K][j]\n",
        "      else:\n",
        "        #print(K, V)\n",
        "        SpamPredProb_var *= (1.0 - map_Prob_Feature_Given_Spam[K])\n",
        "        HamPredProb_var *= (1.0 - map_Prob_Feature_Given_Ham[K])\n",
        "\n",
        "    Spam_Pred.append(SpamPredProb_var)\n",
        "    Ham_Pred.append(HamPredProb_var)\n",
        "\n",
        "    MN_Spam_Pred.append(MN_SpamPredProb_var)\n",
        "    MN_HamPred.append(MN_HamPredProb_var)\n",
        "\n",
        "    TF_Ham_Pred.append(TF_HamPredProb_var)\n",
        "    TF_Spam_Pred.append(TF_SpamPredProb_var)\n",
        "\n",
        "    if SpamPredProb_var>=HamPredProb_var:\n",
        "      Pred_Ouput.append(1)\n",
        "      #print(i)\n",
        "    else:\n",
        "      Pred_Ouput.append(0)  \n",
        "\n",
        "    if MN_SpamPredProb_var>= MN_HamPredProb_var:\n",
        "      MN_Pred_Ouput.append(1)\n",
        "    else:\n",
        "      MN_Pred_Ouput.append(0)\n",
        "    \n",
        "    if TF_SpamPredProb_var>= TF_HamPredProb_var:\n",
        "      TF_Pred_Output.append(1)\n",
        "    else:\n",
        "      TF_Pred_Output.append(0)\n",
        "\n",
        "    #print(len(Pred_Ouput))\n",
        "\n",
        "  CorrectPred = 0\n",
        "  for i in range(filenum):\n",
        "    if Pred_Ouput[i] ==  ActualOutput[i]:\n",
        "      CorrectPred+=1\n",
        "\n",
        "    # for i,j in zip(MN_Pred_Ouput, Pred_Ouput):\n",
        "    #   print(i,j)\n",
        "\n",
        "  MN_CorrectPred = 0\n",
        "  for i in range(filenum):\n",
        "    if MN_Pred_Ouput[i] ==  ActualOutput[i]:\n",
        "      MN_CorrectPred+=1\n",
        "   \n",
        "  TF_CorrectPred = 0\n",
        "  for i in range(filenum):\n",
        "    if TF_Pred_Output[i] ==  ActualOutput[i]:\n",
        "      TF_CorrectPred+=1\n",
        "\n",
        "  #print(Spam_Pred)\n",
        "  \n",
        "  print(f\"---------Feature Length {len(TopNfeatures)} ----------\")\n",
        "  print(\"\\n---------Accuracy Score---------------\")\n",
        "  print(f\"Feature Length {len(TopNfeatures)} Accuracy Bernoulli NB: \", float(CorrectPred/filenum)*100.0)\n",
        "  print(f\"Feature Length {len(TopNfeatures)} Accuracy Multinomial NB: \", float(MN_CorrectPred/filenum)*100.0)\n",
        "  print(f\"Feature Length {len(TopNfeatures)} Accuracy Multinomial NB TF: \", float(TF_CorrectPred/filenum)*100.0)\n",
        "  print(\"\\n---------Precision Score--------------\")\n",
        "  print(f\"Feature Length {len(TopNfeatures)} Precision Bernoulli NB: \", precision_score(ActualOutput, Pred_Ouput))\n",
        "  print(f\"Feature Length {len(TopNfeatures)} Precision Multinomial NB: \", precision_score(ActualOutput, MN_Pred_Ouput))\n",
        "  print(f\"Feature Length {len(TopNfeatures)} Precision Multinomial NB TF: \", precision_score(ActualOutput, TF_Pred_Output))\n",
        "  print(\"\\n---------Recall Score-----------------\")\n",
        "  print(f\"Feature Length {len(TopNfeatures)} Recall Bernoulli NB: \", recall_score(ActualOutput, Pred_Ouput))\n",
        "  print(f\"Feature Length {len(TopNfeatures)} Recall Multinomial NB: \", recall_score(ActualOutput, MN_Pred_Ouput))\n",
        "  print(f\"Feature Length {len(TopNfeatures)} Recall Multinomial NB TF: \", recall_score(ActualOutput, TF_Pred_Output))\n",
        "  print(\"\\n---------Confusion Matrix-------------\")\n",
        "  print(\"Bernoulli NB: \")\n",
        "  print(confusion_matrix(ActualOutput, Pred_Ouput))\n",
        "  print(\"Multinomial NB: \")\n",
        "  print(confusion_matrix(ActualOutput, MN_Pred_Ouput))\n",
        "  print(\"Multinomial NB TF: \")\n",
        "  print(confusion_matrix(ActualOutput, TF_Pred_Output))\n",
        "  print(\"\\n-------Classification Report----------\")\n",
        "  print(\"Bernoulli NB: \")\n",
        "  print(classification_report(ActualOutput, Pred_Ouput))\n",
        "  print(\"Multinomial NB: \")\n",
        "  print(classification_report(ActualOutput, MN_Pred_Ouput))\n",
        "  print(\"Multinomial NB TF: \")\n",
        "  print(classification_report(ActualOutput, TF_Pred_Output)) \n",
        "  print(\"#######################################################\")\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------Feature Length 10 ----------\n",
            "\n",
            "---------Accuracy Score---------------\n",
            "Feature Length 10 Accuracy Bernoulli NB:  96.21993127147766\n",
            "Feature Length 10 Accuracy Multinomial NB:  96.21993127147766\n",
            "Feature Length 10 Accuracy Multinomial NB TF:  97.2508591065292\n",
            "\n",
            "---------Precision Score--------------\n",
            "Feature Length 10 Precision Bernoulli NB:  0.9318181818181818\n",
            "Feature Length 10 Precision Multinomial NB:  0.9318181818181818\n",
            "Feature Length 10 Precision Multinomial NB TF:  0.9555555555555556\n",
            "\n",
            "---------Recall Score-----------------\n",
            "Feature Length 10 Recall Bernoulli NB:  0.8367346938775511\n",
            "Feature Length 10 Recall Multinomial NB:  0.8367346938775511\n",
            "Feature Length 10 Recall Multinomial NB TF:  0.8775510204081632\n",
            "\n",
            "---------Confusion Matrix-------------\n",
            "Bernoulli NB: \n",
            "[[239   3]\n",
            " [  8  41]]\n",
            "Multinomial NB: \n",
            "[[239   3]\n",
            " [  8  41]]\n",
            "Multinomial NB TF: \n",
            "[[240   2]\n",
            " [  6  43]]\n",
            "\n",
            "-------Classification Report----------\n",
            "Bernoulli NB: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98       242\n",
            "           1       0.93      0.84      0.88        49\n",
            "\n",
            "    accuracy                           0.96       291\n",
            "   macro avg       0.95      0.91      0.93       291\n",
            "weighted avg       0.96      0.96      0.96       291\n",
            "\n",
            "Multinomial NB: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98       242\n",
            "           1       0.93      0.84      0.88        49\n",
            "\n",
            "    accuracy                           0.96       291\n",
            "   macro avg       0.95      0.91      0.93       291\n",
            "weighted avg       0.96      0.96      0.96       291\n",
            "\n",
            "Multinomial NB TF: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.98       242\n",
            "           1       0.96      0.88      0.91        49\n",
            "\n",
            "    accuracy                           0.97       291\n",
            "   macro avg       0.97      0.93      0.95       291\n",
            "weighted avg       0.97      0.97      0.97       291\n",
            "\n",
            "#######################################################\n",
            "---------Feature Length 100 ----------\n",
            "\n",
            "---------Accuracy Score---------------\n",
            "Feature Length 100 Accuracy Bernoulli NB:  95.53264604810997\n",
            "Feature Length 100 Accuracy Multinomial NB:  97.59450171821305\n",
            "Feature Length 100 Accuracy Multinomial NB TF:  91.40893470790378\n",
            "\n",
            "---------Precision Score--------------\n",
            "Feature Length 100 Precision Bernoulli NB:  1.0\n",
            "Feature Length 100 Precision Multinomial NB:  0.92\n",
            "Feature Length 100 Precision Multinomial NB TF:  0.6764705882352942\n",
            "\n",
            "---------Recall Score-----------------\n",
            "Feature Length 100 Recall Bernoulli NB:  0.7346938775510204\n",
            "Feature Length 100 Recall Multinomial NB:  0.9387755102040817\n",
            "Feature Length 100 Recall Multinomial NB TF:  0.9387755102040817\n",
            "\n",
            "---------Confusion Matrix-------------\n",
            "Bernoulli NB: \n",
            "[[242   0]\n",
            " [ 13  36]]\n",
            "Multinomial NB: \n",
            "[[238   4]\n",
            " [  3  46]]\n",
            "Multinomial NB TF: \n",
            "[[220  22]\n",
            " [  3  46]]\n",
            "\n",
            "-------Classification Report----------\n",
            "Bernoulli NB: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.97       242\n",
            "           1       1.00      0.73      0.85        49\n",
            "\n",
            "    accuracy                           0.96       291\n",
            "   macro avg       0.97      0.87      0.91       291\n",
            "weighted avg       0.96      0.96      0.95       291\n",
            "\n",
            "Multinomial NB: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99       242\n",
            "           1       0.92      0.94      0.93        49\n",
            "\n",
            "    accuracy                           0.98       291\n",
            "   macro avg       0.95      0.96      0.96       291\n",
            "weighted avg       0.98      0.98      0.98       291\n",
            "\n",
            "Multinomial NB TF: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.91      0.95       242\n",
            "           1       0.68      0.94      0.79        49\n",
            "\n",
            "    accuracy                           0.91       291\n",
            "   macro avg       0.83      0.92      0.87       291\n",
            "weighted avg       0.93      0.91      0.92       291\n",
            "\n",
            "#######################################################\n",
            "---------Feature Length 1000 ----------\n",
            "\n",
            "---------Accuracy Score---------------\n",
            "Feature Length 1000 Accuracy Bernoulli NB:  94.50171821305841\n",
            "Feature Length 1000 Accuracy Multinomial NB:  98.96907216494846\n",
            "Feature Length 1000 Accuracy Multinomial NB TF:  46.735395189003434\n",
            "\n",
            "---------Precision Score--------------\n",
            "Feature Length 1000 Precision Bernoulli NB:  1.0\n",
            "Feature Length 1000 Precision Multinomial NB:  1.0\n",
            "Feature Length 1000 Precision Multinomial NB TF:  0.23232323232323232\n",
            "\n",
            "---------Recall Score-----------------\n",
            "Feature Length 1000 Recall Bernoulli NB:  0.673469387755102\n",
            "Feature Length 1000 Recall Multinomial NB:  0.9387755102040817\n",
            "Feature Length 1000 Recall Multinomial NB TF:  0.9387755102040817\n",
            "\n",
            "---------Confusion Matrix-------------\n",
            "Bernoulli NB: \n",
            "[[242   0]\n",
            " [ 16  33]]\n",
            "Multinomial NB: \n",
            "[[242   0]\n",
            " [  3  46]]\n",
            "Multinomial NB TF: \n",
            "[[ 90 152]\n",
            " [  3  46]]\n",
            "\n",
            "-------Classification Report----------\n",
            "Bernoulli NB: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      1.00      0.97       242\n",
            "           1       1.00      0.67      0.80        49\n",
            "\n",
            "    accuracy                           0.95       291\n",
            "   macro avg       0.97      0.84      0.89       291\n",
            "weighted avg       0.95      0.95      0.94       291\n",
            "\n",
            "Multinomial NB: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99       242\n",
            "           1       1.00      0.94      0.97        49\n",
            "\n",
            "    accuracy                           0.99       291\n",
            "   macro avg       0.99      0.97      0.98       291\n",
            "weighted avg       0.99      0.99      0.99       291\n",
            "\n",
            "Multinomial NB TF: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.37      0.54       242\n",
            "           1       0.23      0.94      0.37        49\n",
            "\n",
            "    accuracy                           0.47       291\n",
            "   macro avg       0.60      0.66      0.45       291\n",
            "weighted avg       0.84      0.47      0.51       291\n",
            "\n",
            "#######################################################\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSdeIxe8PxlN"
      },
      "source": [
        "## Attacker's Strategy; False Negative Rate Before and After Attacker's Strategy Modified test emails "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOujLyZFP72U",
        "outputId": "d0b4a768-0dbe-47d1-a550-8c5107ce2f0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from math import log\n",
        "Spam_Pred = []\n",
        "Ham_Pred = []\n",
        "Pred_Output = []\n",
        "AS_Spam_Pred = []\n",
        "AS_Ham_Pred = []\n",
        "AS_Pred_Output = []\n",
        "ActualOutput = [SpamArry[i] for i in range(TotalEmails-filenum, TotalEmails)]\n",
        "modifications = [0 for i in range(filenum)]\n",
        "Total_SpamsModified = 0\n",
        "Total_modifications = 0\n",
        "FeaGvnHam = {}\n",
        "FeaGvnSpam = {}\n",
        "LO_of_Feature = {}\n",
        "Ctr = 0\n",
        "for K,V in map_Prob_Feature_Given_Ham.items():\n",
        "  if Ctr <10:\n",
        "    FeaGvnHam[K] = V\n",
        "  Ctr+=1\n",
        "Ctr = 0\n",
        "for K,V in map_Prob_Feature_Given_Spam.items():\n",
        "  if Ctr <10:\n",
        "    FeaGvnSpam[K] = V\n",
        "  Ctr+=1\n",
        "\n",
        "\n",
        "for Feature in Top10features:\n",
        "  LO_of_Feature[Feature] = log(float(FeaGvnSpam[Feature]/FeaGvnHam[Feature]),10)\n",
        "#print(LO_of_Feature)\n",
        "\n",
        "sorted_LO_of_Feature =  {k:v for k,v in sorted(LO_of_Feature.items(), key=lambda kv: kv[1])}\n",
        "\n",
        "#print(sorted_FeaGvnHam)\n",
        "Modification_Index = 0\n",
        "Test_DataSet_Index_Ctr = 0\n",
        "for j in range(TotalEmails-filenum, TotalEmails):\n",
        "    FeatureBits = {}\n",
        "    LO_GvntestEmail = 0\n",
        "    for i in Top10features:\n",
        "      if int(DataSet[i][j]) == 1:\n",
        "        FeatureBits[i] = 1   \n",
        "      else:\n",
        "        FeatureBits[i] = 0\n",
        "    original_length = 0\n",
        "    for K,V in FeatureBits.items():\n",
        "      if V == 1:\n",
        "        LO_GvntestEmail += LO_of_Feature[K]\n",
        "        original_length+=1\n",
        "    \n",
        "    new_Feature_length = original_length\n",
        "    ModCount = 0\n",
        "    AdaptModified = False\n",
        "    AS_LO_GvntestEmail = LO_GvntestEmail\n",
        "    #AS_HamPredProb_var = HamPredProb_var\n",
        "    if AS_LO_GvntestEmail > 0:\n",
        "      Pred_Output.append(1)\n",
        "      # in order to only apply attacker's strategy to spam Emails, this condition checks the same.\n",
        "      if ActualOutput[Test_DataSet_Index_Ctr] == 1:\n",
        "        for K,Val in sorted_LO_of_Feature.items():\n",
        "          if FeatureBits[K] == 0:\n",
        "            AS_LO_GvntestEmail += sorted_LO_of_Feature[K]\n",
        "            ModCount+=1\n",
        "            new_Feature_length += 1\n",
        "          if AS_LO_GvntestEmail < 0:\n",
        "            modifications[Modification_Index] = ModCount\n",
        "            Total_SpamsModified+=1\n",
        "            Total_modifications+=ModCount\n",
        "            #print(\"Original Feature Length: \", original_length, \"New Feature Length: \", new_Feature_length)\n",
        "            AdaptModified = True\n",
        "            break\n",
        "    else:\n",
        "      Pred_Output.append(0)\n",
        "\n",
        "    Modification_Index+=1\n",
        "    Test_DataSet_Index_Ctr+=1\n",
        "\n",
        "    if AdaptModified:\n",
        "      AS_Pred_Output.append(0)\n",
        "    else:\n",
        "      AS_Pred_Output.append(Pred_Output[len(Pred_Output)-1])\n",
        "\n",
        "\n",
        "print(\"Confusion Matrix of Vanilla Multinomial NB: \")\n",
        "tn, fp, fn, tp = confusion_matrix(ActualOutput, Pred_Output).ravel()\n",
        "#print(\"False Negatives: \", fn)\n",
        "#print(\"False positive: \", fp)\n",
        "#print(\"True Negatives: \", tn)\n",
        "FNR = fn/(tp+fn)\n",
        "\n",
        "print(confusion_matrix(ActualOutput, Pred_Output))\n",
        "print(\"----------------------------\")\n",
        "print(\"False Negative Rate: \", FNR)\n",
        "print(\"----------------------------\")\n",
        "print(classification_report(ActualOutput, Pred_Output))\n",
        "print(\"Confusion Matrix after applying Attackers Function on test emails; Output: \")\n",
        "tn, fp, fn, tp = confusion_matrix(ActualOutput, AS_Pred_Output).ravel()\n",
        "FNR = fn/(tp+fn)\n",
        "print(confusion_matrix(ActualOutput, AS_Pred_Output))\n",
        "print(\"----------------------------\")\n",
        "print(\"False Negative Rate: \", FNR)\n",
        "print(\"----------------------------\")\n",
        "print(\"----------------------------\")\n",
        "print(\"Cost: \", float(Total_modifications/Total_SpamsModified))\n",
        "print(\"----------------------------\")\n",
        "print(classification_report(ActualOutput, AS_Pred_Output))\n",
        "#print(modifications)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix of Vanilla Multinomial NB: \n",
            "[[236   6]\n",
            " [  3  46]]\n",
            "----------------------------\n",
            "False Negative Rate:  0.061224489795918366\n",
            "----------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98       242\n",
            "           1       0.88      0.94      0.91        49\n",
            "\n",
            "    accuracy                           0.97       291\n",
            "   macro avg       0.94      0.96      0.95       291\n",
            "weighted avg       0.97      0.97      0.97       291\n",
            "\n",
            "Confusion Matrix after applying Attackers Function on test emails; Output: \n",
            "[[236   6]\n",
            " [ 49   0]]\n",
            "----------------------------\n",
            "False Negative Rate:  1.0\n",
            "----------------------------\n",
            "----------------------------\n",
            "Cost:  1.7391304347826086\n",
            "----------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.98      0.90       242\n",
            "           1       0.00      0.00      0.00        49\n",
            "\n",
            "    accuracy                           0.81       291\n",
            "   macro avg       0.41      0.49      0.45       291\n",
            "weighted avg       0.69      0.81      0.74       291\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZfRD9mU6L4k"
      },
      "source": [
        "\n",
        "\n",
        "##   Defender's Strategy\n",
        "Passing the Attackers function/Strategy to Classifier - In other words Train Classifier on Traning Dataset altered/modified using Attackers Function/Strategy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILvx8Dgh6K9B",
        "outputId": "eb81deb9-1807-42aa-d9c2-aedad6193832",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Total_modifications = 0\n",
        "Total_SpamsModified = 0\n",
        "Zpy_DataSet = DataSet.copy(deep=True)\n",
        "Modification_Index = 0\n",
        "filenum = 291\n",
        "modifications = [0 for i in range(len(Zpy_DataSet[\"SPAM\"]))]\n",
        "for j in range(0,(len(Zpy_DataSet[\"SPAM\"])-filenum)):\n",
        "    FeatureBits = {}\n",
        "    LO_of_trainEmail = 0\n",
        "    for i in Top10features:\n",
        "      if int(Zpy_DataSet[i][j]) == 1:\n",
        "        FeatureBits[i] = 1   \n",
        "      else:\n",
        "        FeatureBits[i] = 0\n",
        "    \n",
        "    #print('-----------------------------------------')\n",
        "    for K,V in FeatureBits.items():\n",
        "        if V == 1:\n",
        "          Zpy_DataSet[K][j] == 1\n",
        "          #print(K) \n",
        "    original_length = 0\n",
        "    for K,V in FeatureBits.items():\n",
        "      if V == 1:\n",
        "        LO_of_trainEmail += LO_of_Feature[K]\n",
        "        original_length += 1\n",
        "\n",
        "    AS_LO_of_trainEmail = LO_of_trainEmail\n",
        "    Mod_count = 0\n",
        "    AdaptModified = False\n",
        "    new_Feature_length = original_length\n",
        "    if AS_LO_of_trainEmail > 0: \n",
        "      # in order to only apply attacker's strategy to spam Emails, this condition checks the same.\n",
        "      if Zpy_DataSet[\"SPAM\"][j] == 1:\n",
        "        for K,Val in sorted_LO_of_Feature.items():\n",
        "          if FeatureBits[K] == 0:\n",
        "            AS_LO_of_trainEmail += sorted_LO_of_Feature[K]\n",
        "            Mod_count+=1\n",
        "            FeatureBits[K] = 1\n",
        "            new_Feature_length += 1\n",
        "          if AS_LO_of_trainEmail < 0:\n",
        "            modifications[Modification_Index] = Mod_count\n",
        "            Total_SpamsModified+=1\n",
        "            Total_modifications+=Mod_count\n",
        "            #print(\"Original Feature Length: \", original_length, \"New Feature Length: \", new_Feature_length)\n",
        "            AdaptModified = True\n",
        "            break    \n",
        "  \n",
        "    Modification_Index+=1\n",
        "    if AdaptModified:\n",
        "      for K,V in FeatureBits.items():\n",
        "        if V == 1:\n",
        "          Zpy_DataSet[K][j] = 1\n",
        "          #print(K)     \n",
        "    #print('-----------------------------------------')\n",
        "    \n",
        "# if Total_SpamsModified != 0:\n",
        "#    print(float(Total_modifications/Total_SpamsModified))\n",
        "\n",
        "print(\"Total Spam Mails Modified: \", Total_SpamsModified)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Spam Mails Modified:  397\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IijWtPUvW1_i"
      },
      "source": [
        "Calculating New values of Feature Instances after Defender's Strategy "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UvbD2UFO1vU",
        "outputId": "c5d064d3-3013-4919-da70-4300e7bba113",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Part 1 to - Part 9 training \n",
        "\n",
        "#part 10 total files\n",
        "filenum = 291\n",
        "\n",
        "map_Feature_Given_Spam = {}\n",
        "map_Feature_Given_Ham = {}\n",
        " \n",
        "DataSetLen = len(Zpy_DataSet[\"SPAM\"])\n",
        "#print(\"DataSetLen\",DataSetLen)\n",
        "SpamArry = [0] * DataSetLen\n",
        "SpamArry = [int(sub) for sub in Zpy_DataSet[\"SPAM\"]] \n",
        "\n",
        "count = 0\n",
        "\n",
        "for i in SpamArry:\n",
        "  if i == 1:\n",
        "    count+=1\n",
        "\n",
        "Legitp = float((DataSetLen - count)/DataSetLen)\n",
        "Legit_Count = (DataSetLen - count)\n",
        "#print(\"Legit_Count \",Legit_Count)\n",
        "Spam_Count = count\n",
        "#print(\"Spam_Count \", Spam_Count)\n",
        "#print(Legitp)\n",
        "spamp = float(count/DataSetLen)\n",
        "#print(spamp)\n",
        "\n",
        "# Selecting the highest Feature list\n",
        "for k in Top10features:\n",
        "  map_Feature_Given_Spam[k] = 0\n",
        "  map_Feature_Given_Ham[k] = 0\n",
        "\n",
        "#Map of Features for 1 to 9 parts\n",
        "for i in Zpy_DataSet:\n",
        "  if i in Top10features:\n",
        "    for j in range(0,(len(Zpy_DataSet[i])-filenum)):\n",
        "      if int(Zpy_DataSet[i][j]) == 1 and int(Zpy_DataSet[\"SPAM\"][j]) == 1:\n",
        "        map_Feature_Given_Spam[i]+=1\n",
        "      elif int(Zpy_DataSet[i][j]) == 1 and int(Zpy_DataSet[\"SPAM\"][j]) == 0:\n",
        "        map_Feature_Given_Ham[i]+=1\n",
        "\n",
        "print(map_Feature_Given_Ham)\n",
        "print(map_Feature_Given_Spam)\n",
        "\n",
        "# Spam_Count and Legit_Count - part10 outputs \n",
        "TotalEmails = len(SpamArry)\n",
        "SpamCnt1to9prt = Spam_Count\n",
        "HamCnt1to9prt = Legit_Count\n",
        "TotalTest = 0\n",
        "for i in range(TotalEmails-filenum,TotalEmails):\n",
        "  TotalTest +=1\n",
        "  if SpamArry[i] == 0:\n",
        "    HamCnt1to9prt -= 1\n",
        "  else:\n",
        "    SpamCnt1to9prt -= 1 \n",
        "\n",
        "print(\" Spam_Count\",Spam_Count)\n",
        "print(\" Legit_Count\",Legit_Count)\n",
        "#print(\" TotalTest\",TotalTest)\n",
        "print(\" Spam Count 1 to 9 part\",SpamCnt1to9prt)\n",
        "print(\" Ham Count 1 to 9 part\",HamCnt1to9prt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'!': 337, 'language': 1447, 'remove': 22, 'free': 129, 'university': 1276, 'linguistic': 1105, 'money': 58, 'click': 13, '@': 1564, 'our': 367}\n",
            "{'!': 364, 'language': 245, 'remove': 185, 'free': 251, 'university': 116, 'linguistic': 397, 'money': 169, 'click': 116, '@': 116, 'our': 260}\n",
            " Spam_Count 481\n",
            " Legit_Count 2412\n",
            " Spam Count 1 to 9 part 432\n",
            " Ham Count 1 to 9 part 2170\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCmsin8JRNDK"
      },
      "source": [
        "Calculating New weights/ Feature Probabilities after applying Defender's Strategy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDroJa9WOi2v"
      },
      "source": [
        "map_Prob_Feature_Given_Ham = {}\n",
        "map_Prob_Feature_Given_Spam = {k:float((v+1)/(SpamCnt1to9prt+2)) for k,v in map_Feature_Given_Spam.items()}\n",
        "for Key,Val in map_Feature_Given_Ham.items():\n",
        "  map_Prob_Feature_Given_Ham[Key] = float((Val+1)/(HamCnt1to9prt+2))\n",
        "\n",
        "#print(map_Prob_Feature_Given_Ham)\n",
        "#print(map_Prob_Feature_Given_Spam)\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFl81Gi4S40y"
      },
      "source": [
        "## Applying Defender's strategy trained Classifier on (Modified through attacker's strategy) test DataSet and checking the False Positive and False Negative Rate\n",
        "\n",
        "*   While the Classifier is adept at classifying Spam Mails as Spam - which is suggested by Lower False Negative Rate, after tuning training data with Defender's strategy.\n",
        "\n",
        "*   While it drastically improves on False Negativity, but it deteriorates its ability to categories Ham mails as Ham effectively. This is evident in the False Positives recorded.  \n",
        "\n",
        "*   Due to the above the accuracy of the model is hampered, but as a result of this activity the model is more robust and less overfitting.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmIiHSCCFEh4",
        "outputId": "8e703787-4897-401c-ff5c-97cfe2b8edc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "\n",
        "len(ActualOutput)\n",
        "\n",
        "Pred_Ouput = []\n",
        "Spam_Pred = []\n",
        "Ham_Pred = []\n",
        "\n",
        "MN_Spam_Pred = []\n",
        "MN_HamPred = []\n",
        "MN_Pred_Ouput = []\n",
        "\n",
        "TF_Pred_Output = []\n",
        "TF_Ham_Pred = []\n",
        "TF_Spam_Pred = []\n",
        "for j in range(TotalEmails-filenum, TotalEmails):\n",
        "  FeatureBits = {}\n",
        "  for i in Top10features:\n",
        "      if int(Zpy_DataSet[i][j]) == 1:\n",
        "        FeatureBits[i] = 1   \n",
        "      else:\n",
        "        FeatureBits[i] = 0\n",
        "\n",
        "  SpamPredProb_var = 1.0*Prob_Spam1to9\n",
        "  HamPredProb_var = 1.0*Prob_Ham1to9\n",
        "  \n",
        "  for K,V in FeatureBits.items():\n",
        "    if V == 1:\n",
        "      SpamPredProb_var *= map_Prob_Feature_Given_Spam[K]\n",
        "      HamPredProb_var *= map_Prob_Feature_Given_Ham[K]\n",
        "    \n",
        "  if SpamPredProb_var>=HamPredProb_var:\n",
        "    Pred_Ouput.append(1)\n",
        "    #print(i)\n",
        "  else:\n",
        "    Pred_Ouput.append(0)\n",
        "\n",
        "print(\"Multinomial NB after Defender's Strategy: \")\n",
        "tn, fp, fn, tp = confusion_matrix(ActualOutput, Pred_Ouput).ravel()\n",
        "FNR = fn/(tp+fn)\n",
        "print(\"Confusion Matrix: \")\n",
        "print(confusion_matrix(ActualOutput, Pred_Output))\n",
        "print(\"----------------------------\")\n",
        "print(\"False Negative Rate: \", FNR)\n",
        "print(\"----------------------------\")\n",
        "FPR = fp/(tn+fp)\n",
        "print(\"----------------------------\")\n",
        "print(\"False Positive Rate: \", FPR)\n",
        "print(\"----------------------------\")\n",
        "print(\"----------------------------\")\n",
        "print(\"Accuracy Score: \", (tn+tp)/(filenum) * 100)\n",
        "print(\"----------------------------\")\n",
        "print(classification_report(ActualOutput,Pred_Ouput))\n",
        "#print(Class)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Multinomial NB after Defender's Strategy: \n",
            "Confusion Matrix: \n",
            "[[236   6]\n",
            " [  3  46]]\n",
            "----------------------------\n",
            "False Negative Rate:  0.16326530612244897\n",
            "----------------------------\n",
            "----------------------------\n",
            "False Positive Rate:  0.13636363636363635\n",
            "----------------------------\n",
            "----------------------------\n",
            "Accuracy Score:  85.91065292096219\n",
            "----------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.86      0.91       242\n",
            "           1       0.55      0.84      0.67        49\n",
            "\n",
            "    accuracy                           0.86       291\n",
            "   macro avg       0.76      0.85      0.79       291\n",
            "weighted avg       0.89      0.86      0.87       291\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZE-Ii1JoEwP"
      },
      "source": [
        "In the Above Output one can see that the flase Negative Rate has fallen considerably indicating that defender's startegy is working well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl2V9hZNiaHQ"
      },
      "source": [
        "## SVM Model Based Spam Filtering\n",
        "\n",
        "\n",
        "1.   Used Grid Search CV and Random Search CV for parameters tuning. \n",
        "2.   Grid Search CV's accuracy is marginally better than Randomised Search CV's\n",
        "3.   I am using the SVC() function provided by sklearn library\n",
        "4.   I am using the parameters like Kernel, C, Coef0 and Gamma for training the SVM model. Beacause these are the most important parameters that affect the accuracy by considerable amount. If we select an inappropriate kernel the accuracy is hampered by a huge margin.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUxBT2YpgZvh"
      },
      "source": [
        "Training and validation data Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHCWKu9cDC1i"
      },
      "source": [
        "from sklearn import svm, datasets\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "New_DataSet = DataSet.drop([i for i in range(len(DataSet[\"SPAM\"])-291, len(DataSet))], axis=0)\n",
        "X_Train = New_DataSet[Top10features]\n",
        "Y_Train = New_DataSet[\"SPAM\"]\n",
        "#print(Y_Train)\n",
        "#print(X_Train)\n",
        "New_DataSet_2 = DataSet.drop([i for i in range(0, len(DataSet[\"SPAM\"])-291)], axis=0)\n",
        "#print(New_DataSet_2)\n",
        "X_test = New_DataSet_2[Top10features]\n",
        "Y_test = New_DataSet_2[\"SPAM\"]\n",
        "#print(X_test)\n",
        "#print(Y_test)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYZwrvB-ggQD"
      },
      "source": [
        "Finding the best of Parameters from ***Kernel, C, Coef0, Degree and Gamma*** using Grid Search CV\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8Or9C0XgXkJ",
        "outputId": "fa625761-6a81-4016-a1fc-4d7f31bbc482",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "parameters = {'kernel':('linear', 'poly', 'rbf','sigmoid'), \n",
        "              'C':[1,50,5],\n",
        "              'degree':[3,8],\n",
        "              'coef0':[0.001,10,0.5],\n",
        "              'gamma':('auto','scale')}\n",
        "\n",
        "SVModel = svm.SVC()\n",
        "GridSrch = GridSearchCV(SVModel, parameters, cv = 3)\n",
        "GridSrch.fit(X_Train,Y_Train)\n",
        "GridSrch.best_params_"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'C': 5, 'coef0': 0.001, 'degree': 3, 'gamma': 'auto', 'kernel': 'rbf'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xlJsNrLgzEa"
      },
      "source": [
        "Getting ***Accuracy*** of Model based on Grid Search CV On validation/Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eDsfBZsigNT",
        "outputId": "813bebdd-4e2c-4f7d-a71d-53fff0c698ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Using the best parameter values from above\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "SVModel = svm.SVC( kernel='rbf', C=5, degree=3, gamma = 'auto', coef0 = 0.001)\n",
        "SVModel.fit(X_Train,Y_Train)\n",
        "print(\"Accuracy: \")\n",
        "print(accuracy_score(Y_test, SVModel.predict(X_test)))\n",
        "print(\"-------------------------------\")\n",
        "print(\"Precision SVM : \", precision_score(ActualOutput, SVModel.predict(X_test)))\n",
        "print(\"-------------------------------\")\n",
        "print(\"Recall SVM: \", precision_score(ActualOutput, SVModel.predict(X_test)))\n",
        "print(\"-------------------------------\")  "
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: \n",
            "0.9656357388316151\n",
            "-------------------------------\n",
            "Precision SVM :  0.9534883720930233\n",
            "-------------------------------\n",
            "Recall SVM:  0.9534883720930233\n",
            "-------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ3GpyF6gN_j"
      },
      "source": [
        "Finding the best of Parameters from ***Kernel, C, Coef0, Degree and Gamma*** using Randomized Search CV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc4BUjpGgK9k",
        "outputId": "59540571-aaae-4049-e419-011829beace2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "RandS = RandomizedSearchCV(SVModel, parameters, cv = 3)\n",
        "RandS.fit(X_Train, Y_Train)\n",
        "RandS.best_params_"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'C': 50, 'coef0': 0.001, 'degree': 3, 'gamma': 'auto', 'kernel': 'rbf'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn96lNVWhDu6"
      },
      "source": [
        "Getting **Accuracy** of Model based on Randomized Search CV parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To21WTAQGcdG",
        "outputId": "ac1fce2b-f646-4a86-da7e-e7d522a0835e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "SVModel = svm.SVC( kernel='rbf', C=50, degree=3, gamma = 'auto', coef0 = 0.001)\n",
        "SVModel.fit(X_Train,Y_Train)\n",
        "print(\"Accuracy: \")\n",
        "print(accuracy_score(Y_test, SVModel.predict(X_test)))\n",
        "print(\"-------------------------------\")\n",
        "print(\"Precision SVM : \", precision_score(ActualOutput, SVModel.predict(X_test)))\n",
        "print(\"-------------------------------\")\n",
        "print(\"Recall SVM: \", precision_score(ActualOutput, SVModel.predict(X_test)))\n",
        "print(\"-------------------------------\")  "
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: \n",
            "0.9621993127147767\n",
            "-------------------------------\n",
            "Precision SVM :  0.9523809523809523\n",
            "-------------------------------\n",
            "Recall SVM:  0.9523809523809523\n",
            "-------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCzZYsj7fp6M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}